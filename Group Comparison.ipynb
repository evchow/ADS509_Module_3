{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the â€œAssignment Materialsâ€ section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "# below are defined functions that will create a pipeline to normalize and tokenize while keeping # and emojis\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison; a set of all punctuation\n",
    "tw_punct = punctuation - {\"#\"} # this omits the hashtag from the punctuation set\n",
    "\n",
    "# define stopwords\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Two useful regex\n",
    "# first to ID whitespace and second to ID hashtags\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "# UNICODE_EMOJI is deprecated; use is_emoji instead\n",
    "# in order to recode, important to understand that this is a function to add emojis in a list\n",
    "# defined in is_emoji to all_language_emojis set\n",
    "emoji_list = []\n",
    "for country in emoji_list: \n",
    "    for em in emoji.is_emoji[country]: \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    # Place your Module 2 solution here\n",
    "    num_tokens = len(tokens) # len gets the sum of values in a list\n",
    "    # see https://www.geeksforgeeks.org/how-to-count-unique-values-inside-a-list/\n",
    "    num_unique_tokens = len(set(tokens)) # sets don't contain duplicates, therefore we can sum a set to get unique values\n",
    "    lexical_diversity = len(set(tokens))/len(tokens)\n",
    "    # see https://stackoverflow.com/questions/25934586/finding-the-amount-of-characters-of-all-words-in-a-list-in-python\n",
    "    num_characters = sum([len(i) for i in tokens])\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "\n",
    "        # print the five most common tokens\n",
    "        # see https://www.geeksforgeeks.org/find-k-frequent-words-data-set-python/\n",
    "        print(Counter(tokens).most_common(5))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "# lexical diversity is a measure of the number of unique words in a text\n",
    "# lexical diversity = number of unique words/ total number of words   \n",
    "# code from descriptive_stats defined function taken from Module 2 Assignment, cell #4\n",
    "\n",
    "# see slack from Prof. Marbut for change\n",
    "def is_emoji(s):\n",
    "    return(emoji.is_emoji(s))\n",
    "\n",
    "def contains_emoji(s):\n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if is_emoji(ch)]\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "# below is a function to remove stop tokens\n",
    "# see https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return[w for w in tokens if w not in sw]\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    # modify this function to return tokens\n",
    "    # we defined the whitespace_pattern earlier\n",
    "    # we'll have it return i, where i = tokens\n",
    "    return(i for i in whitespace_pattern.split(text))\n",
    "\n",
    "# let's define a lowercase function\n",
    "# see # see https://www.programiz.com/python-programming/methods/string/casefold\n",
    "\n",
    "# def lowercase(text):\n",
    "    # return(text.casefold())\n",
    "    # was going to define lowercase function, but already defined a few cells down\n",
    "\n",
    "# everything ^^^ is to prepare to load into vvv; a pipeline of functions to leave us with\n",
    "# twitter data that includes hashtags and emojis\n",
    "# no need to define a string conversion as it is already provided\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)    \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use the below cells as an example or read in the data in a way you prefer\n",
    "# we'll use the data pulled from Module 1\n",
    "data_location = \"/Users/evachow/Documents/GitHub/ADS509/ADS509_Module_1/\"\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'mychemicalromance':'MCRofficial_followers.txt',\n",
    "                'missy':'MissyElliott_followers.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df415d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-eabbbca4aefa>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['mychemicalromance'],\n",
      "b'Skipping line 75449: expected 7 fields, saw 8\\nSkipping line 76732: expected 7 fields, saw 8\\nSkipping line 86300: expected 7 fields, saw 11\\nSkipping line 107413: expected 7 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['mychemicalromance'],\n",
    "                           sep='\\t',\n",
    "                           lineterminator='\\n', # added because couldn't parse?\n",
    "                           error_bad_lines=False, # some lines saw additional fields?\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"mychemicalromance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966804cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f3b47281c5dd>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['missy'],\n",
      "b'Skipping line 66847: expected 7 fields, saw 8\\nSkipping line 69573: expected 7 fields, saw 8\\nSkipping line 73464: expected 7 fields, saw 23\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['missy'],\n",
    "                             sep='\\t',\n",
    "                             lineterminator='\\n',\n",
    "                             error_bad_lines=False,\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"missy\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659041e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132811754176765952</td>\n",
       "      <td>nutman71234668</td>\n",
       "      <td>quaintqueef420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>i smelly</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1570075902817583106</td>\n",
       "      <td>DemoLoversMCR</td>\n",
       "      <td>Demolition Lovers Gang - MCR</td>\n",
       "      <td>Newark, New Jersey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Official petition. We want to hear Demoition L...</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537290582556454914</td>\n",
       "      <td>Meooowcy</td>\n",
       "      <td>CrÃ©amyLattÃ©</td>\n",
       "      <td>Lungsod ng Valenzuela, Pambans</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232241829191593984</td>\n",
       "      <td>jamieexisted</td>\n",
       "      <td>jamie</td>\n",
       "      <td>he/him, 19</td>\n",
       "      <td>33.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1570087687683555328</td>\n",
       "      <td>KarmenWeaks</td>\n",
       "      <td>Karmen Weaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username                          name  \\\n",
       "0  1132811754176765952  nutman71234668                quaintqueef420   \n",
       "1  1570075902817583106   DemoLoversMCR  Demolition Lovers Gang - MCR   \n",
       "2  1537290582556454914        Meooowcy                   CrÃ©amyLattÃ©   \n",
       "3  1232241829191593984    jamieexisted                         jamie   \n",
       "4  1570087687683555328     KarmenWeaks                  Karmen Weaks   \n",
       "\n",
       "                         location  follower_count  following  \\\n",
       "0                             NaN            26.0      509.0   \n",
       "1              Newark, New Jersey             1.0        6.0   \n",
       "2  Lungsod ng Valenzuela, Pambans             2.0       65.0   \n",
       "3                      he/him, 19            33.0      310.0   \n",
       "4                             NaN             0.0       89.0   \n",
       "\n",
       "                                         description             artist  \n",
       "0                                           i smelly  mychemicalromance  \n",
       "1  Official petition. We want to hear Demoition L...  mychemicalromance  \n",
       "2                                                NaN  mychemicalromance  \n",
       "3                                          ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸  mychemicalromance  \n",
       "4                                                NaN  mychemicalromance  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the twitter data dataframes\n",
    "twitter_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics here\n",
    "# this is taken from assignment submitted for Module 2 with modifications\n",
    "# define our unique artists\n",
    "artist = ['mychemicalromance', 'missy']\n",
    "\n",
    "# define the path (define the lyric folder then the folder for each artist)\n",
    "path_lyrics = data_location + lyrics_folder\n",
    "\n",
    "# see https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "path_lyrics_artist = [f for f in os.listdir(path_lyrics) if not f.startswith('.')]\n",
    "\n",
    "# let's append artist, song, and lyrics of each song\n",
    "singer = [] # defined as singer because will probably use artist for loops later\n",
    "song = [] # DON'T FORGET THAT THIS (SINGULAR) IS A LIST\n",
    "lyric = []\n",
    "\n",
    "# first, a loop to go through each artist\n",
    "for artist in path_lyrics_artist:\n",
    "    unique_artist = path_lyrics + '/' + artist\n",
    "    unique_song = os.listdir(unique_artist)\n",
    "    \n",
    "    # now a loop to go through each artist's song\n",
    "    for music in unique_song:\n",
    "        # hidden .DS_Store file needs to be hidden otherwise [Errno 20]\n",
    "        path_songs = unique_artist + '/' + music\n",
    "        # see https://www.adamsmith.haus/python/answers/how-to-read-a-text-file-into-a-list-in-python\n",
    "        with open(path_songs) as the_path:\n",
    "            the_songs = the_path.readlines()\n",
    "            \n",
    "        singer.append(artist)\n",
    "        song.append(the_songs[0])\n",
    "        lyric.append(''.join(the_songs[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920afe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics\\n</td>\n",
       "      <td>\\n\\n\\nGravity don't mean too much to me\\nI'm w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics\\n</td>\n",
       "      <td>\\n\\n\\nAnd we can run from the backdrop of thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics\\n</td>\n",
       "      <td>\\n\\n\\nWe could be perfect one last night\\nAnd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics\\n</td>\n",
       "      <td>\\n\\n\\nYou're not in this alone, let me break t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics\\n</td>\n",
       "      <td>\\n\\n\\n\"They're, they're these terrors, and it'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                          song  \\\n",
       "0  mychemicalromance  My Chemical Romance Lyrics\\n   \n",
       "1  mychemicalromance  My Chemical Romance Lyrics\\n   \n",
       "2  mychemicalromance  My Chemical Romance Lyrics\\n   \n",
       "3  mychemicalromance  My Chemical Romance Lyrics\\n   \n",
       "4  mychemicalromance  My Chemical Romance Lyrics\\n   \n",
       "\n",
       "                                               lyric  \n",
       "0  \\n\\n\\nGravity don't mean too much to me\\nI'm w...  \n",
       "1  \\n\\n\\nAnd we can run from the backdrop of thes...  \n",
       "2  \\n\\n\\nWe could be perfect one last night\\nAnd ...  \n",
       "3  \\n\\n\\nYou're not in this alone, let me break t...  \n",
       "4  \\n\\n\\n\"They're, they're these terrors, and it'...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe to check read in\n",
    "headers = {'artist': singer,\n",
    "          'song': song,\n",
    "          'lyric': lyric}\n",
    "lyrics_data = pd.DataFrame(headers)\n",
    "lyrics_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e8b11",
   "metadata": {},
   "source": [
    "Turns out that song titles weren't pulled properly, but this won't be a problem as we won't be using this for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4541c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics</td>\n",
       "      <td>Gravity don't mean too much to me I'm who I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics</td>\n",
       "      <td>And we can run from the backdrop of these g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics</td>\n",
       "      <td>We could be perfect one last night And die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics</td>\n",
       "      <td>You're not in this alone, let me break this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>My Chemical Romance Lyrics</td>\n",
       "      <td>\"They're, they're these terrors, and it's l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                         song  \\\n",
       "0  mychemicalromance  My Chemical Romance Lyrics    \n",
       "1  mychemicalromance  My Chemical Romance Lyrics    \n",
       "2  mychemicalromance  My Chemical Romance Lyrics    \n",
       "3  mychemicalromance  My Chemical Romance Lyrics    \n",
       "4  mychemicalromance  My Chemical Romance Lyrics    \n",
       "\n",
       "                                               lyric  \n",
       "0     Gravity don't mean too much to me I'm who I...  \n",
       "1     And we can run from the backdrop of these g...  \n",
       "2     We could be perfect one last night And die ...  \n",
       "3     You're not in this alone, let me break this...  \n",
       "4     \"They're, they're these terrors, and it's l...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \\n or tabs that have appeared\n",
    "lyrics_data = lyrics_data.replace(r'\\n', ' ', regex=True)\n",
    "lyrics_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "* Remove stopwords (optional)\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "* Remove stopwords\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "# we'll apply the pipeline functions to both, so stopwords will be removed for twitter and lyrics data\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyric\"].apply(prepare, pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc06e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>has_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132811754176765952</td>\n",
       "      <td>nutman71234668</td>\n",
       "      <td>quaintqueef420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>i smelly</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[smelly]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1570075902817583106</td>\n",
       "      <td>DemoLoversMCR</td>\n",
       "      <td>Demolition Lovers Gang - MCR</td>\n",
       "      <td>Newark, New Jersey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Official petition. We want to hear Demoition L...</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[official, petition, want, hear, demoition, lo...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537290582556454914</td>\n",
       "      <td>Meooowcy</td>\n",
       "      <td>CrÃ©amyLattÃ©</td>\n",
       "      <td>Lungsod ng Valenzuela, Pambans</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232241829191593984</td>\n",
       "      <td>jamieexisted</td>\n",
       "      <td>jamie</td>\n",
       "      <td>he/him, 19</td>\n",
       "      <td>33.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1570087687683555328</td>\n",
       "      <td>KarmenWeaks</td>\n",
       "      <td>Karmen Weaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username                          name  \\\n",
       "0  1132811754176765952  nutman71234668                quaintqueef420   \n",
       "1  1570075902817583106   DemoLoversMCR  Demolition Lovers Gang - MCR   \n",
       "2  1537290582556454914        Meooowcy                   CrÃ©amyLattÃ©   \n",
       "3  1232241829191593984    jamieexisted                         jamie   \n",
       "4  1570087687683555328     KarmenWeaks                  Karmen Weaks   \n",
       "\n",
       "                         location  follower_count  following  \\\n",
       "0                             NaN            26.0      509.0   \n",
       "1              Newark, New Jersey             1.0        6.0   \n",
       "2  Lungsod ng Valenzuela, Pambans             2.0       65.0   \n",
       "3                      he/him, 19            33.0      310.0   \n",
       "4                             NaN             0.0       89.0   \n",
       "\n",
       "                                         description             artist  \\\n",
       "0                                           i smelly  mychemicalromance   \n",
       "1  Official petition. We want to hear Demoition L...  mychemicalromance   \n",
       "2                                                NaN  mychemicalromance   \n",
       "3                                          ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸  mychemicalromance   \n",
       "4                                                NaN  mychemicalromance   \n",
       "\n",
       "                                              tokens  num_tokens  has_emoji  \n",
       "0                                           [smelly]           1      False  \n",
       "1  [official, petition, want, hear, demoition, lo...          15      False  \n",
       "2                                              [nan]           1      False  \n",
       "3                                        [ğŸ³ï¸â€ğŸŒˆğŸ³ï¸â€âš§ï¸]           1       True  \n",
       "4                                              [nan]           1      False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102293</th>\n",
       "      <td>missy</td>\n",
       "      <td>ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿</td>\n",
       "      <td>[ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29938</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>amo a mimizinha â¤ï¸</td>\n",
       "      <td>[amo, mimizinha, â¤ï¸]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109364</th>\n",
       "      <td>missy</td>\n",
       "      <td>Producer ğŸ“€ ğŸ‡¨ğŸ‡­</td>\n",
       "      <td>[producer, ğŸ“€, ğŸ‡¨ğŸ‡­]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89017</th>\n",
       "      <td>missy</td>\n",
       "      <td>â¤ï¸twerk queen // Taurus // 17ğŸ’</td>\n",
       "      <td>[â¤ï¸twerk, queen, taurus, 17ğŸ’]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87693</th>\n",
       "      <td>missy</td>\n",
       "      <td>Empowering peopleğŸ’œ</td>\n",
       "      <td>[empowering, peopleğŸ’œ]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82704</th>\n",
       "      <td>missy</td>\n",
       "      <td>ğŸ«€ğŸ¥ğŸ¬ğŸ§¬ğŸ›¸ğŸ«ğŸ˜ğŸ‘¾ğŸ®ğŸğŸ¹ğŸ¼ğŸ¸âœˆï¸ğŸ“±ğŸ“¸ğŸ’»ğŸ”ªâ›“âœ‰ï¸ğŸ–‹ğŸ†’ğŸ¶ğŸ”Šâ˜€ï¸â­ï¸âš¡ï¸ğŸŒŠğŸŒˆğŸŒŒğŸ¤¸ğŸ»â€â™€ï¸ğŸ½ğŸ“ğŸ”¥ğŸŒµğŸ‘ğŸ‘½...</td>\n",
       "      <td>[ğŸ«€ğŸ¥ğŸ¬ğŸ§¬ğŸ›¸ğŸ«ğŸ˜ğŸ‘¾ğŸ®ğŸğŸ¹ğŸ¼ğŸ¸âœˆï¸ğŸ“±ğŸ“¸ğŸ’»ğŸ”ªâ›“âœ‰ï¸ğŸ–‹ğŸ†’ğŸ¶ğŸ”Šâ˜€ï¸â­ï¸âš¡ï¸ğŸŒŠğŸŒˆğŸŒŒğŸ¤¸ğŸ»â€â™€ï¸ğŸ½ğŸ“ğŸ”¥ğŸŒµğŸ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81286</th>\n",
       "      <td>missy</td>\n",
       "      <td>âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨ ğŸ’° ğŸ”¥ğŸŒ¶ï¸ I am an official membe...</td>\n",
       "      <td>[âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨, ğŸ’°, ğŸ”¥ğŸŒ¶ï¸, official, member, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81131</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>FamilyğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ğŸ’™aspiring music journalist. LiveJ...</td>\n",
       "      <td>[familyğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ğŸ’™aspiring, music, journalist, li...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70505</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>dickgrabbers + corpse ğŸ¥°</td>\n",
       "      <td>[dickgrabbers, corpse, ğŸ¥°]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47423</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"No.5,Hook,Mando,Dean W,Steve H,AmazingğŸ•·,Jacob...</td>\n",
       "      <td>[no5hookmandodean, wsteve, hamazingğŸ•·jacob, fry...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   artist                                        description  \\\n",
       "102293              missy                              ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿   \n",
       "29938   mychemicalromance                                 amo a mimizinha â¤ï¸   \n",
       "109364              missy                                      Producer ğŸ“€ ğŸ‡¨ğŸ‡­   \n",
       "89017               missy                     â¤ï¸twerk queen // Taurus // 17ğŸ’   \n",
       "87693               missy                                 Empowering peopleğŸ’œ   \n",
       "82704               missy  ğŸ«€ğŸ¥ğŸ¬ğŸ§¬ğŸ›¸ğŸ«ğŸ˜ğŸ‘¾ğŸ®ğŸğŸ¹ğŸ¼ğŸ¸âœˆï¸ğŸ“±ğŸ“¸ğŸ’»ğŸ”ªâ›“âœ‰ï¸ğŸ–‹ğŸ†’ğŸ¶ğŸ”Šâ˜€ï¸â­ï¸âš¡ï¸ğŸŒŠğŸŒˆğŸŒŒğŸ¤¸ğŸ»â€â™€ï¸ğŸ½ğŸ“ğŸ”¥ğŸŒµğŸ‘ğŸ‘½...   \n",
       "81286               missy  âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨ ğŸ’° ğŸ”¥ğŸŒ¶ï¸ I am an official membe...   \n",
       "81131   mychemicalromance  FamilyğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ğŸ’™aspiring music journalist. LiveJ...   \n",
       "70505   mychemicalromance                            dickgrabbers + corpse ğŸ¥°   \n",
       "47423   mychemicalromance  \"No.5,Hook,Mando,Dean W,Steve H,AmazingğŸ•·,Jacob...   \n",
       "\n",
       "                                                   tokens  num_tokens  \n",
       "102293                            [ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿]           1  \n",
       "29938                                [amo, mimizinha, â¤ï¸]           3  \n",
       "109364                                  [producer, ğŸ“€, ğŸ‡¨ğŸ‡­]           3  \n",
       "89017                       [â¤ï¸twerk, queen, taurus, 17ğŸ’]           4  \n",
       "87693                               [empowering, peopleğŸ’œ]           2  \n",
       "82704   [ğŸ«€ğŸ¥ğŸ¬ğŸ§¬ğŸ›¸ğŸ«ğŸ˜ğŸ‘¾ğŸ®ğŸğŸ¹ğŸ¼ğŸ¸âœˆï¸ğŸ“±ğŸ“¸ğŸ’»ğŸ”ªâ›“âœ‰ï¸ğŸ–‹ğŸ†’ğŸ¶ğŸ”Šâ˜€ï¸â­ï¸âš¡ï¸ğŸŒŠğŸŒˆğŸŒŒğŸ¤¸ğŸ»â€â™€ï¸ğŸ½ğŸ“ğŸ”¥ğŸŒµğŸ‘...           1  \n",
       "81286   [âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨âœ¨, ğŸ’°, ğŸ”¥ğŸŒ¶ï¸, official, member, ...          17  \n",
       "81131   [familyğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ğŸ’™aspiring, music, journalist, li...           5  \n",
       "70505                           [dickgrabbers, corpse, ğŸ¥°]           3  \n",
       "47423   [no5hookmandodean, wsteve, hamazingğŸ•·jacob, fry...           5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\",\"num_tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da96123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>When I was a young boy, my father Took me i...</td>\n",
       "      <td>[, young, boy, father, took, city, see, marchi...</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>And if they get me and the sun goes down in...</td>\n",
       "      <td>[, get, sun, goes, ground, get, take, spike, h...</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>missy</td>\n",
       "      <td>I know your not my man That you belong to s...</td>\n",
       "      <td>[, know, man, belong, someone, else, nah, come...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"Better step off now\" she said But believe ...</td>\n",
       "      <td>[, better, step, said, believe, sometimes, tho...</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>missy</td>\n",
       "      <td>[Missy] This is for my ghetto motherfuckers...</td>\n",
       "      <td>[, missy, ghetto, motherfuckers, uh, hey, real...</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>missy</td>\n",
       "      <td>Party Time (Uh uh) Uh (Say what?) Uh huh (Y...</td>\n",
       "      <td>[, party, time, uh, uh, uh, say, uh, huh, yo, ...</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>The amount of pills I'm taking Counteracts ...</td>\n",
       "      <td>[, amount, pills, im, taking, counteracts, boo...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Well, it rains and it pours when you're out...</td>\n",
       "      <td>[, well, rains, pours, youre, crash, couch, sl...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Gaze into her killing jar, I'd sometimes st...</td>\n",
       "      <td>[, gaze, killing, jar, id, sometimes, stare, h...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>missy</td>\n",
       "      <td>This is a Missy Elliott exclusive   I keep ...</td>\n",
       "      <td>[, missy, elliott, exclusive, keep, coming, ba...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                artist                                              lyric  \\\n",
       "34   mychemicalromance     When I was a young boy, my father Took me i...   \n",
       "41   mychemicalromance     And if they get me and the sun goes down in...   \n",
       "225              missy     I know your not my man That you belong to s...   \n",
       "67   mychemicalromance     \"Better step off now\" she said But believe ...   \n",
       "203              missy     [Missy] This is for my ghetto motherfuckers...   \n",
       "219              missy     Party Time (Uh uh) Uh (Say what?) Uh huh (Y...   \n",
       "17   mychemicalromance     The amount of pills I'm taking Counteracts ...   \n",
       "102  mychemicalromance     Well, it rains and it pours when you're out...   \n",
       "50   mychemicalromance     Gaze into her killing jar, I'd sometimes st...   \n",
       "196              missy     This is a Missy Elliott exclusive   I keep ...   \n",
       "\n",
       "                                                tokens  num_tokens  \n",
       "34   [, young, boy, father, took, city, see, marchi...         251  \n",
       "41   [, get, sun, goes, ground, get, take, spike, h...         244  \n",
       "225  [, know, man, belong, someone, else, nah, come...         131  \n",
       "67   [, better, step, said, believe, sometimes, tho...         176  \n",
       "203  [, missy, ghetto, motherfuckers, uh, hey, real...         269  \n",
       "219  [, party, time, uh, uh, uh, say, uh, huh, yo, ...         188  \n",
       "17   [, amount, pills, im, taking, counteracts, boo...         210  \n",
       "102  [, well, rains, pours, youre, crash, couch, sl...         173  \n",
       "50   [, gaze, killing, jar, id, sometimes, stare, h...         140  \n",
       "196  [, missy, elliott, exclusive, keep, coming, ba...         185  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_tok_nom = lyrics_data[['artist', 'lyric', 'tokens', 'num_tokens']]\n",
    "lyrics_tok_nom.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: A consecutive series of emojis is identified as one single token. Given patience and time, the emoji sequence could be broken up (much like letters of a word) so that each emoji would be represented as a token. In addition, we can see that a consecutive series of text and emoji together with no spacing also presents itself as one single token. This could be adjusted so that the text would be separated from the emoji, with each counting as a token. In addition, the nature of Missy Elliott's lyrics may require a customized list of stopwords to include the ommission of other \"filler\" words in her lyrics such as \"uh\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
