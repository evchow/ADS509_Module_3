{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "# below are defined functions that will create a pipeline to normalize and tokenize while keeping # and emojis\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison; a set of all punctuation\n",
    "tw_punct = punctuation - {\"#\"} # this omits the hashtag from the punctuation set\n",
    "\n",
    "# define stopwords\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Two useful regex\n",
    "# first to ID whitespace and second to ID hashtags\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "# UNICODE_EMOJI is deprecated; use is_emoji instead\n",
    "# in order to recode, important to understand that this is a function to add emojis in a list\n",
    "# defined in is_emoji to all_language_emojis set\n",
    "emoji_list = []\n",
    "for country in emoji_list: \n",
    "    for em in emoji.is_emoji[country]: \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    # Place your Module 2 solution here\n",
    "    num_tokens = len(tokens) # len gets the sum of values in a list\n",
    "    # see https://www.geeksforgeeks.org/how-to-count-unique-values-inside-a-list/\n",
    "    num_unique_tokens = len(set(tokens)) # sets don't contain duplicates, therefore we can sum a set to get unique values\n",
    "    lexical_diversity = len(set(tokens))/len(tokens)\n",
    "    # see https://stackoverflow.com/questions/25934586/finding-the-amount-of-characters-of-all-words-in-a-list-in-python\n",
    "    num_characters = sum([len(i) for i in tokens])\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "\n",
    "        # print the five most common tokens\n",
    "        # see https://www.geeksforgeeks.org/find-k-frequent-words-data-set-python/\n",
    "        print(Counter(tokens).most_common(11))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "# lexical diversity is a measure of the number of unique words in a text\n",
    "# lexical diversity = number of unique words/ total number of words   \n",
    "# code from descriptive_stats defined function taken from Module 2 Assignment, cell #4\n",
    "\n",
    "# see slack from Prof. Marbut for change\n",
    "def is_emoji(s):\n",
    "    return(emoji.is_emoji(s))\n",
    "\n",
    "def contains_emoji(s):\n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if is_emoji(ch)]\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "# below is a function to remove stop tokens\n",
    "# see https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return[w for w in tokens if w not in sw]\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    # modify this function to return tokens\n",
    "    # we defined the whitespace_pattern earlier\n",
    "    # we'll have it return i, where i = tokens\n",
    "    return(i for i in whitespace_pattern.split(text))\n",
    "\n",
    "# let's define a lowercase function\n",
    "# see https://www.programiz.com/python-programming/methods/string/casefold\n",
    "\n",
    "# def lowercase(text):\n",
    "    # return(text.casefold())\n",
    "    # was going to define lowercase function, but already defined a few cells down\n",
    "\n",
    "# everything ^^^ is to prepare to load into vvv; a pipeline of functions to leave us with\n",
    "# twitter data that includes hashtags and emojis\n",
    "# no need to define a string conversion as it is already provided\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)    \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use the below cells as an example or read in the data in a way you prefer\n",
    "# we'll use the data pulled from Module 1\n",
    "data_location = \"/Users/evachow/Documents/GitHub/ADS509/ADS509_Module_1/\"\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'mychemicalromance':'MCRofficial_followers.txt',\n",
    "                'missy':'MissyElliott_followers.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df415d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-851813fe3bef>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['mychemicalromance'],\n",
      "b'Skipping line 75449: expected 7 fields, saw 8\\nSkipping line 76732: expected 7 fields, saw 8\\nSkipping line 86300: expected 7 fields, saw 11\\nSkipping line 107413: expected 7 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['mychemicalromance'],\n",
    "                           sep='\\t',\n",
    "                           lineterminator='\\n', # added because couldn't parse\n",
    "                           error_bad_lines=False, # some lines saw additional fields; this ignores those rows\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"mychemicalromance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966804cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f3b47281c5dd>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['missy'],\n",
      "b'Skipping line 66847: expected 7 fields, saw 8\\nSkipping line 69573: expected 7 fields, saw 8\\nSkipping line 73464: expected 7 fields, saw 23\\n'\n"
     ]
    }
   ],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['missy'],\n",
    "                             sep='\\t',\n",
    "                             lineterminator='\\n',\n",
    "                             error_bad_lines=False,\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"missy\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9524b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132811754176765952</td>\n",
       "      <td>nutman71234668</td>\n",
       "      <td>quaintqueef420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>i smelly</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1570075902817583106</td>\n",
       "      <td>DemoLoversMCR</td>\n",
       "      <td>Demolition Lovers Gang - MCR</td>\n",
       "      <td>Newark, New Jersey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Official petition. We want to hear Demoition L...</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537290582556454914</td>\n",
       "      <td>Meooowcy</td>\n",
       "      <td>CréamyLatté</td>\n",
       "      <td>Lungsod ng Valenzuela, Pambans</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232241829191593984</td>\n",
       "      <td>jamieexisted</td>\n",
       "      <td>jamie</td>\n",
       "      <td>he/him, 19</td>\n",
       "      <td>33.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>🏳️‍🌈🏳️‍⚧️</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1570087687683555328</td>\n",
       "      <td>KarmenWeaks</td>\n",
       "      <td>Karmen Weaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username                          name  \\\n",
       "0  1132811754176765952  nutman71234668                quaintqueef420   \n",
       "1  1570075902817583106   DemoLoversMCR  Demolition Lovers Gang - MCR   \n",
       "2  1537290582556454914        Meooowcy                   CréamyLatté   \n",
       "3  1232241829191593984    jamieexisted                         jamie   \n",
       "4  1570087687683555328     KarmenWeaks                  Karmen Weaks   \n",
       "\n",
       "                         location  follower_count  following  \\\n",
       "0                             NaN            26.0      509.0   \n",
       "1              Newark, New Jersey             1.0        6.0   \n",
       "2  Lungsod ng Valenzuela, Pambans             2.0       65.0   \n",
       "3                      he/him, 19            33.0      310.0   \n",
       "4                             NaN             0.0       89.0   \n",
       "\n",
       "                                         description             artist  \n",
       "0                                           i smelly  mychemicalromance  \n",
       "1  Official petition. We want to hear Demoition L...  mychemicalromance  \n",
       "2                                                NaN  mychemicalromance  \n",
       "3                                          🏳️‍🌈🏳️‍⚧️  mychemicalromance  \n",
       "4                                                NaN  mychemicalromance  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the twitter data dataframes\n",
    "twitter_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics here\n",
    "# this is taken from assignment submitted for Module 2 with modifications\n",
    "# define our unique artists\n",
    "artist = ['mychemicalromance', 'missy']\n",
    "\n",
    "# define the path (define the lyric folder then the folder for each artist)\n",
    "path_lyrics = data_location + lyrics_folder\n",
    "\n",
    "# see https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "path_lyrics_artist = [f for f in os.listdir(path_lyrics) if not f.startswith('.')]\n",
    "\n",
    "# let's append artist, song, and lyrics of each song\n",
    "singer = [] # defined as singer because will probably use artist for loops later\n",
    "lyric = []\n",
    "\n",
    "# first, a loop to go through each artist\n",
    "for artist in path_lyrics_artist:\n",
    "    unique_artist = path_lyrics + '/' + artist\n",
    "    unique_song = os.listdir(unique_artist)\n",
    "    \n",
    "    # now a loop to go through each artist's song\n",
    "    for music in unique_song:\n",
    "        path_songs = unique_artist + '/' + music\n",
    "        # see https://www.adamsmith.haus/python/answers/how-to-read-a-text-file-into-a-list-in-python\n",
    "        with open(path_songs) as the_path:\n",
    "            the_songs = the_path.readlines()\n",
    "            \n",
    "        singer.append(artist)\n",
    "        lyric.append(''.join(the_songs[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77ed0e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\\n\\n\\nGravity don't mean too much to me\\nI'm w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\\n\\n\\nAnd we can run from the backdrop of thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\\n\\n\\nWe could be perfect one last night\\nAnd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\\n\\n\\nYou're not in this alone, let me break t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\\n\\n\\n\"They're, they're these terrors, and it'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                                              lyric\n",
       "0  mychemicalromance  \\n\\n\\nGravity don't mean too much to me\\nI'm w...\n",
       "1  mychemicalromance  \\n\\n\\nAnd we can run from the backdrop of thes...\n",
       "2  mychemicalromance  \\n\\n\\nWe could be perfect one last night\\nAnd ...\n",
       "3  mychemicalromance  \\n\\n\\nYou're not in this alone, let me break t...\n",
       "4  mychemicalromance  \\n\\n\\n\"They're, they're these terrors, and it'..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe to check read in\n",
    "headers = {'artist': singer,\n",
    "          'lyric': lyric}\n",
    "lyrics_data = pd.DataFrame(headers)\n",
    "lyrics_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02194aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Gravity don't mean too much to me I'm who I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>And we can run from the backdrop of these g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>We could be perfect one last night And die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>You're not in this alone, let me break this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"They're, they're these terrors, and it's l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                                              lyric\n",
       "0  mychemicalromance     Gravity don't mean too much to me I'm who I...\n",
       "1  mychemicalromance     And we can run from the backdrop of these g...\n",
       "2  mychemicalromance     We could be perfect one last night And die ...\n",
       "3  mychemicalromance     You're not in this alone, let me break this...\n",
       "4  mychemicalromance     \"They're, they're these terrors, and it's l..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \\n or tabs that have appeared\n",
    "lyrics_data = lyrics_data.replace(r'\\n', ' ', regex=True)\n",
    "lyrics_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "* Remove stopwords (optional)\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "* Remove stopwords\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "# we'll apply the pipeline functions to both, so stopwords will be removed for twitter and lyrics data\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyric\"].apply(prepare, pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5baced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>has_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132811754176765952</td>\n",
       "      <td>nutman71234668</td>\n",
       "      <td>quaintqueef420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>i smelly</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[smelly]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1570075902817583106</td>\n",
       "      <td>DemoLoversMCR</td>\n",
       "      <td>Demolition Lovers Gang - MCR</td>\n",
       "      <td>Newark, New Jersey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Official petition. We want to hear Demoition L...</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[official, petition, want, hear, demoition, lo...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537290582556454914</td>\n",
       "      <td>Meooowcy</td>\n",
       "      <td>CréamyLatté</td>\n",
       "      <td>Lungsod ng Valenzuela, Pambans</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232241829191593984</td>\n",
       "      <td>jamieexisted</td>\n",
       "      <td>jamie</td>\n",
       "      <td>he/him, 19</td>\n",
       "      <td>33.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>🏳️‍🌈🏳️‍⚧️</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[🏳️‍🌈🏳️‍⚧️]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1570087687683555328</td>\n",
       "      <td>KarmenWeaks</td>\n",
       "      <td>Karmen Weaks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username                          name  \\\n",
       "0  1132811754176765952  nutman71234668                quaintqueef420   \n",
       "1  1570075902817583106   DemoLoversMCR  Demolition Lovers Gang - MCR   \n",
       "2  1537290582556454914        Meooowcy                   CréamyLatté   \n",
       "3  1232241829191593984    jamieexisted                         jamie   \n",
       "4  1570087687683555328     KarmenWeaks                  Karmen Weaks   \n",
       "\n",
       "                         location  follower_count  following  \\\n",
       "0                             NaN            26.0      509.0   \n",
       "1              Newark, New Jersey             1.0        6.0   \n",
       "2  Lungsod ng Valenzuela, Pambans             2.0       65.0   \n",
       "3                      he/him, 19            33.0      310.0   \n",
       "4                             NaN             0.0       89.0   \n",
       "\n",
       "                                         description             artist  \\\n",
       "0                                           i smelly  mychemicalromance   \n",
       "1  Official petition. We want to hear Demoition L...  mychemicalromance   \n",
       "2                                                NaN  mychemicalromance   \n",
       "3                                          🏳️‍🌈🏳️‍⚧️  mychemicalromance   \n",
       "4                                                NaN  mychemicalromance   \n",
       "\n",
       "                                              tokens  num_tokens  has_emoji  \n",
       "0                                           [smelly]           1      False  \n",
       "1  [official, petition, want, hear, demoition, lo...          15      False  \n",
       "2                                              [nan]           1      False  \n",
       "3                                        [🏳️‍🌈🏳️‍⚧️]           1       True  \n",
       "4                                              [nan]           1      False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53374</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>•Hiberno🇮🇪•Bláthadóir •Metalhead •Leabharbhách...</td>\n",
       "      <td>[•hiberno🇮🇪•bláthadóir, •metalhead, •leabharbh...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32707</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>happy pride month!!!!!!🌈. single . bisexual😘</td>\n",
       "      <td>[happy, pride, month🌈, single, bisexual😘]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54437</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>LULA 2022 ❤️</td>\n",
       "      <td>[lula, 2022, ❤️]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57577</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"𓂃🥛ᵎ / randomacc.𝖼𝗈𝗆 𖥦⌕﹅</td>\n",
       "      <td>[𓂃🥛ᵎ, randomacc𝖼𝗈𝗆, 𖥦⌕﹅]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28207</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>𝐌𝐲 𝐂𝐡𝐞𝐦𝐢𝐜𝐚𝐥 𝐑𝐨𝐦𝐚𝐧𝐜𝐞 𝐍𝐞𝐯𝐞𝐫 𝐃𝐞𝐚𝐭𝐡 a̶n̶d̶ W̶e̶ a̶...</td>\n",
       "      <td>[𝐌𝐲, 𝐂𝐡𝐞𝐦𝐢𝐜𝐚𝐥, 𝐑𝐨𝐦𝐚𝐧𝐜𝐞, 𝐍𝐞𝐯𝐞𝐫, 𝐃𝐞𝐚𝐭𝐡, a̶n̶d̶, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69348</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"𝓢𝓵𝔂𝓽𝓱𝓮𝓻𝓲𝓷 🐍💚</td>\n",
       "      <td>[𝓢𝓵𝔂𝓽𝓱𝓮𝓻𝓲𝓷, 🐍💚]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31366</th>\n",
       "      <td>missy</td>\n",
       "      <td>🟫▫️ | Him 1st Λlwαyѕ | No Ξxceptions |▫️🟫Multi...</td>\n",
       "      <td>[🟫▫️, 1st, λlwαyѕ, ξxceptions, ▫️🟫multiaward, ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58122</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>\"Respiratory Therapist 🩺</td>\n",
       "      <td>[respiratory, therapist, 🩺]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59904</th>\n",
       "      <td>missy</td>\n",
       "      <td>Let good things follow you ♥️❤️</td>\n",
       "      <td>[let, good, things, follow, ♥️❤️]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16071</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>表面是医学生内在却是超级瑟情淫乱的反差RBQ～哎嘿，欢迎进来💗 雌堕日记记录中～个人🐧272...</td>\n",
       "      <td>[表面是医学生内在却是超级瑟情淫乱的反差rbq～哎嘿，欢迎进来💗, 雌堕日记记录中～个人🐧2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  artist                                        description  \\\n",
       "53374  mychemicalromance  •Hiberno🇮🇪•Bláthadóir •Metalhead •Leabharbhách...   \n",
       "32707  mychemicalromance       happy pride month!!!!!!🌈. single . bisexual😘   \n",
       "54437  mychemicalromance                                       LULA 2022 ❤️   \n",
       "57577  mychemicalromance                           \"𓂃🥛ᵎ / randomacc.𝖼𝗈𝗆 𖥦⌕﹅   \n",
       "28207  mychemicalromance  𝐌𝐲 𝐂𝐡𝐞𝐦𝐢𝐜𝐚𝐥 𝐑𝐨𝐦𝐚𝐧𝐜𝐞 𝐍𝐞𝐯𝐞𝐫 𝐃𝐞𝐚𝐭𝐡 a̶n̶d̶ W̶e̶ a̶...   \n",
       "69348  mychemicalromance                                      \"𝓢𝓵𝔂𝓽𝓱𝓮𝓻𝓲𝓷 🐍💚   \n",
       "31366              missy  🟫▫️ | Him 1st Λlwαyѕ | No Ξxceptions |▫️🟫Multi...   \n",
       "58122  mychemicalromance                           \"Respiratory Therapist 🩺   \n",
       "59904              missy                    Let good things follow you ♥️❤️   \n",
       "16071  mychemicalromance  表面是医学生内在却是超级瑟情淫乱的反差RBQ～哎嘿，欢迎进来💗 雌堕日记记录中～个人🐧272...   \n",
       "\n",
       "                                                  tokens  num_tokens  \n",
       "53374  [•hiberno🇮🇪•bláthadóir, •metalhead, •leabharbh...          13  \n",
       "32707          [happy, pride, month🌈, single, bisexual😘]           5  \n",
       "54437                                   [lula, 2022, ❤️]           3  \n",
       "57577                           [𓂃🥛ᵎ, randomacc𝖼𝗈𝗆, 𖥦⌕﹅]           3  \n",
       "28207  [𝐌𝐲, 𝐂𝐡𝐞𝐦𝐢𝐜𝐚𝐥, 𝐑𝐨𝐦𝐚𝐧𝐜𝐞, 𝐍𝐞𝐯𝐞𝐫, 𝐃𝐞𝐚𝐭𝐡, a̶n̶d̶, ...          15  \n",
       "69348                                    [𝓢𝓵𝔂𝓽𝓱𝓮𝓻𝓲𝓷, 🐍💚]           2  \n",
       "31366  [🟫▫️, 1st, λlwαyѕ, ξxceptions, ▫️🟫multiaward, ...          16  \n",
       "58122                        [respiratory, therapist, 🩺]           3  \n",
       "59904                  [let, good, things, follow, ♥️❤️]           5  \n",
       "16071  [表面是医学生内在却是超级瑟情淫乱的反差rbq～哎嘿，欢迎进来💗, 雌堕日记记录中～个人🐧2...           2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\",\"num_tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa37410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>missy</td>\n",
       "      <td>[Intro: Timbaland] Yeah, Yeah! Check it... ...</td>\n",
       "      <td>[, intro, timbaland, yeah, yeah, check, yeah, ...</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Yeah Everybody pay attention to me I got th...</td>\n",
       "      <td>[, yeah, everybody, pay, attention, got, answe...</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>missy</td>\n",
       "      <td>[Intro: Missy]  Damn, he he he Timbaland, w...</td>\n",
       "      <td>[, intro, missy, damn, timbaland, bout, lace, ...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>It's the tearing sound of love notes Drowni...</td>\n",
       "      <td>[, tearing, sound, love, notes, drowning, gray...</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>みんあ！ くるま わ まんたん だし すーつけーす に わ ばくだん つめてーいる じ...</td>\n",
       "      <td>[, みんあ！, くるま, わ, まんたん, だし, すーつけーす, に, わ, ばくだん,...</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Whoa, all I want to know All I want  With j...</td>\n",
       "      <td>[, whoa, want, know, want, touch, burning, han...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Love, was just a passing phase A fever that...</td>\n",
       "      <td>[, love, passing, phase, fever, ran, away, til...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>They're gonna clean up your looks with all ...</td>\n",
       "      <td>[, theyre, gonna, clean, looks, lies, books, m...</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>mychemicalromance</td>\n",
       "      <td>Alright, children The lights are out and th...</td>\n",
       "      <td>[, alright, children, lights, partys, time, do...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>missy</td>\n",
       "      <td>We started in 1997 and finished in 1999 Ha,...</td>\n",
       "      <td>[, started, 1997, finished, 1999, ha, ha, brou...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                artist                                              lyric  \\\n",
       "211              missy     [Intro: Timbaland] Yeah, Yeah! Check it... ...   \n",
       "14   mychemicalromance     Yeah Everybody pay attention to me I got th...   \n",
       "194              missy     [Intro: Missy]  Damn, he he he Timbaland, w...   \n",
       "100  mychemicalromance     It's the tearing sound of love notes Drowni...   \n",
       "16   mychemicalromance     みんあ！ くるま わ まんたん だし すーつけーす に わ ばくだん つめてーいる じ...   \n",
       "101  mychemicalromance     Whoa, all I want to know All I want  With j...   \n",
       "81   mychemicalromance     Love, was just a passing phase A fever that...   \n",
       "63   mychemicalromance     They're gonna clean up your looks with all ...   \n",
       "45   mychemicalromance     Alright, children The lights are out and th...   \n",
       "188              missy     We started in 1997 and finished in 1999 Ha,...   \n",
       "\n",
       "                                                tokens  num_tokens  \n",
       "211  [, intro, timbaland, yeah, yeah, check, yeah, ...         420  \n",
       "14   [, yeah, everybody, pay, attention, got, answe...         213  \n",
       "194  [, intro, missy, damn, timbaland, bout, lace, ...         245  \n",
       "100  [, tearing, sound, love, notes, drowning, gray...         208  \n",
       "16   [, みんあ！, くるま, わ, まんたん, だし, すーつけーす, に, わ, ばくだん,...         307  \n",
       "101  [, whoa, want, know, want, touch, burning, han...         101  \n",
       "81   [, love, passing, phase, fever, ran, away, til...         153  \n",
       "63   [, theyre, gonna, clean, looks, lies, books, m...         162  \n",
       "45   [, alright, children, lights, partys, time, do...          51  \n",
       "188  [, started, 1997, finished, 1999, ha, ha, brou...          45  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_tok_nom = lyrics_data[['artist', 'lyric', 'tokens', 'num_tokens']]\n",
    "lyrics_tok_nom.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: A consecutive series of emojis is identified as one single token. Given patience and time, the emoji sequence could be broken up (much like letters of a word) so that each emoji would be represented as a token. In addition, we can see that a consecutive series of text and emoji together with no spacing also presents itself as one single token. This could be adjusted so that the text would be separated from the emoji, with each counting as a token. In addition, the nature of Missy Elliott's lyrics may require a customized list of stopwords to include the ommission of other \"filler\" words in her lyrics such as \"uh\" \"ohhh\" and other lyrics sung to add groove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18a2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following returns a dataframe with tokens for respective artist lyrics data\n",
    "mcr_tokens = lyrics_tok_nom[lyrics_tok_nom['artist'] == 'mychemicalromance']\n",
    "missy_tokens = lyrics_tok_nom[lyrics_tok_nom['artist'] == 'missy']\n",
    "\n",
    "# this results in a column where every row is a list\n",
    "# now we need to make a list for each artists' tokens\n",
    "mcr_tokens_list = []\n",
    "missy_tokens_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a39a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define each of the tokens\n",
    "# extend adds our tokens from the column of lists of tokens for that artist\n",
    "# to the list of all tokens\n",
    "# see https://www.programiz.com/python-programming/methods/list/extend\n",
    "for token in mcr_tokens['tokens']:\n",
    "    mcr_tokens_list.extend(token)\n",
    "    \n",
    "for token in missy_tokens['tokens']:\n",
    "    missy_tokens_list.extend(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36c3513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics for My Chemical Romance lyrics:\n",
      "There are 16458 tokens in the data.\n",
      "There are 2261 unique tokens in the data.\n",
      "There are 76375 characters in the data.\n",
      "The lexical diversity is 0.137 in the data.\n",
      "[('dont', 259), ('im', 252), ('na', 252), ('well', 245), ('never', 237), ('', 208), ('get', 186), ('go', 186), ('like', 168), ('say', 164), ('oh', 160)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16458, 2261, 0.13737999756957103, 76375]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that descriptive_stats function was changed to display top 11 words due to a blank\n",
    "# entry for both artists\n",
    "print('\\nDescriptive statistics for My Chemical Romance lyrics:')\n",
    "descriptive_stats(mcr_tokens_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4ef340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive statistics for Missy Elliott lyrics:\n",
      "There are 33131 tokens in the data.\n",
      "There are 5282 unique tokens in the data.\n",
      "There are 149977 characters in the data.\n",
      "The lexical diversity is 0.159 in the data.\n",
      "[('im', 679), ('like', 600), ('missy', 447), ('dont', 431), ('get', 409), ('got', 361), ('know', 305), ('yeah', 260), ('oh', 257), ('', 250), ('go', 239)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[33131, 5282, 0.1594277262986327, 149977]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nDescriptive statistics for Missy Elliott lyrics:')\n",
    "descriptive_stats(missy_tokens_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: What observations do you make about these data? \n",
    "\n",
    "A: Despite the assumption that as a rapper, Missy Elliott would have a significantly higher lexical diversity compared to My Chemical Romance, the lexical diversity between the two artists is not too far off, with Missy Elliott showing slightly more diverse language with a lexical diversity of 0.16 compared to My Chemical Romance's 0.14. However, what's interesting to see is that Missy Elliott had nearly double the amount of words and unique words as My Chemical Romance.  \n",
    "\n",
    "See https://pudding.cool/projects/vocabulary/index.html for reference for initial thought.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to view the top 10 tokens for each artist and see the concentration of that token in \n",
    "# their respective artist as well as the other artist for their lyrics and follower descriptions\n",
    "\n",
    "# let's get frequency of the top 10 tokens in their own corpus\n",
    "# define a function to get info\n",
    "def corpus_stats(list_of_tokens, verbose=True):\n",
    "    appearances = Counter(list_of_tokens).most_common(10) # this is count of # times the token appears\n",
    "    #perc_freq = appearances / len(list_of_tokens) # this is %freq of word compared to all words in corpus\n",
    "    \n",
    "    print(appearances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c45d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dont', 259), ('im', 252), ('na', 252), ('well', 245), ('never', 237), ('', 208), ('get', 186), ('go', 186), ('like', 168), ('say', 164)]\n"
     ]
    }
   ],
   "source": [
    "corpus_stats(mcr_tokens_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
